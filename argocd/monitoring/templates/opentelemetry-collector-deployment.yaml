{{- if .Values.opentelemetryCollectorDeployment.enabled }}
{{/*
An additional OTel collector deployment in addition to the daemonset we install with the opentelemetry-demo chart.
It is used to collect cluster metrics.
*/}}
---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: opentelemetry-collector-deployment
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  destination:
    namespace: open-telemetry
    name: in-cluster
  project: default
  syncPolicy:
    automated:
{{ if .Values.opentelemetryCollectorDeployment.autoSync }}
      selfHeal: true
{{- end }}
{{- if .Values.opentelemetryCollectorDeployment.prune }}
      prune: true
{{- end }}
    syncOptions:
      - CreateNamespace=true
  source:
    chart: opentelemetry-collector
    repoURL: https://open-telemetry.github.io/opentelemetry-helm-charts
    targetRevision: {{ .Values.opentelemetryCollectorDeployment.version }}
    helm:
      mode: deployment
      replicaCount: 1

      image:
        repository: "otel/opentelemetry-collector-contrib"

      presets:
        # Collect cluster-level metrics from the Kubernetes API server.
        clusterMetrics:
          enabled: true

        # https://opentelemetry.io/docs/kubernetes/helm/collector/#kubernetes-events-preset
        # Collect Kubernetes events.
        kubernetesEvents:
          enabled: true

        kubernetesAttributes:
          enabled: true

      resources:
        limits:
          # default is 200Mi and services often report that they cannot send
          # telemetry due to the memory limiter kicking in and refusing data
          # (message: data refused due to high memory usage)
          memory: 500Mi
      config:
        receivers:
          otlp:
            protocols:
              http:
                # Bind to 0.0.0.0 to allow for `kubectl port-forward` to work
                # This may be susceptible to denial of service attacks - CWE-1327 https://cwe.mitre.org/data/definitions/1327.html
                endpoint: 0.0.0.0:4318
          prometheus:
            config:
              global:
                scrape_interval: 15s
              scrape_configs:
                # Default collector scrape config
                - job_name: opentelemetry-collector
                  scrape_interval: 10s
                  static_configs:
                    - targets:
                        - ${env:MY_POD_IP}:8888

                # This config is used to find all Kubernetes endpoints that have the prometheus.io/scrape annotation.
                # In particular, it will scrape the kube-state-metrics server to provide the kube_... metrics.
                - job_name: kubernetes-service-endpoints
                  # This specifies that Prometheus should discover targets based on Kubernetes endpoints.
                  kubernetes_sd_configs:
                    - role: endpoints
                  relabel_configs:
                    # Keep targets with prometheus.io/scrape annotation set to true
                    - action: keep
                      regex: true
                      source_labels:
                        - __meta_kubernetes_service_annotation_prometheus_io_scrape
                    # Set the scheme (http/https) based on prometheus.io/scheme annotation
                    - action: replace
                      regex: (https?)
                      source_labels:
                        - __meta_kubernetes_service_annotation_prometheus_io_scheme
                      target_label: __scheme__
                    # Set the metrics path based on prometheus.io/path annotation
                    - action: replace
                      regex: (.+)
                      source_labels:
                        - __meta_kubernetes_service_annotation_prometheus_io_path
                      target_label: __metrics_path__
                    # Set the address based on the prometheus.io/port annotation
                    - action: replace
                      regex: (.+?)(?::\d+)?;(\d+)
                      replacement: $$1:$$2
                      source_labels:
                        - __address__
                        - __meta_kubernetes_service_annotation_prometheus_io_port
                      target_label: __address__
                    # Map annotations prefixed with prometheus.io/param_ to Prometheus parameters
                    - action: labelmap
                      regex: __meta_kubernetes_service_annotation_prometheus_io_param_(.+)
                      replacement: __param_$$1
                    # Map service labels to Prometheus labels
                    - action: labelmap
                      regex: __meta_kubernetes_service_label_(.+)
                    # Add namespace label based on the Kubernetes namespace
                    - action: replace
                      source_labels:
                        - __meta_kubernetes_namespace
                      target_label: namespace
                    # Add service label based on the Kubernetes service name
                    - action: replace
                      source_labels:
                        - __meta_kubernetes_service_name
                      target_label: service
                    # Add node label based on the Kubernetes pod node name
                    - action: replace
                      source_labels:
                        - __meta_kubernetes_pod_node_name
                      target_label: node

        extensions:
          bearertokenauth/dash0:
            scheme: "Bearer"
            token: "auth_nkCOzkm8oBqfi90e87o7JAbyOjSZ51Sp"

        exporters:
          "otlp/dash0":
            endpoint: 'ingress.eu-west-1.aws.dash0.com:4317'
            auth:
              authenticator: bearertokenauth/dash0

        processors:
          resourcedetection:
            detectors: ["ec2"]
          resource:
            # Temporarily setting these as hard-coded values to make the demo
            # look a bit nicer, since the AWS resource detectors do not set
            # these attributes yet, see
            # https://linear.app/dash0/issue/ENG-952/collect-aws-resource-attributes
            attributes:
              - key: cloud.account.id
                value: "869266160017"
                action: insert
              - key: cloud.availability_zone
                value: eu-west-1a
                action: insert
              - key: cloud.region
                value: eu-west-1
                action: insert
              - key: cloud.platform
                value: aws_eks
                action: insert
              - key: cloud.provider
                value: aws
                action: insert

        service:
          extensions:
            - health_check
            - memory_ballast
            - bearertokenauth/dash0

          pipelines:
            metrics:
              receivers:
                - otlp
                - k8s_cluster
                - prometheus
              processors:
                - k8sattributes
                - memory_limiter
                - resourcedetection
                - resource
                - batch
              exporters:
                - otlp/dash0

---
# Additional RBAC permissions required for running opentelemetry-collector in deployment mode
# Related to https://linear.app/dash0/issue/ENG-732/failed-to-watch-v1node-failed-to-list-v1node-nodes-is-forbidden
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: opentelemetry-collector-deployment-additional
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: opentelemetry-collector-deployment-additional
subjects:
  - kind: ServiceAccount
    name: opentelemetry-collector-deployment
    namespace: open-telemetry
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: opentelemetry-collector-deployment-additional
rules:
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get
      - watch
      - list

  # Required for kubeletstats receiver
  - apiGroups: [""]
    resources: ["nodes/stats"]
    verbs: ["get"]

  # Required for kubeletstats receiver to retrieve metrics from persistentvolumes, ...
  # https://github.com/honeycombio/field-utils/blob/main/collector-configs/k8s-metrics/03-clusterrole.yaml
  - apiGroups:
      - ""
    resources:
      - events
      - namespaces
      - namespaces/status
      - nodes
      - nodes/metrics
      - nodes/spec
      - nodes/stats
      - nodes/proxy
      - persistentvolumes
      - persistentvolumeclaims
      - pods
      - pods/status
      - replicationcontrollers
      - replicationcontrollers/status
      - resourcequotas
      - services
    verbs:
      - get
      - list
      - watch

  - apiGroups:
      - apps
    resources:
      - daemonsets
      - deployments
      - replicasets
      - statefulsets
    verbs:
      - get
      - list
      - watch

  - apiGroups:
      - extensions
    resources:
      - daemonsets
      - deployments
      - replicasets
    verbs:
      - get
      - list
      - watch

  - apiGroups:
      - batch
    resources:
      - jobs
      - cronjobs
    verbs:
      - get
      - list
      - watch

  - apiGroups:
      - autoscaling
    resources:
      - horizontalpodautoscalers
    verbs:
      - get
      - list
      - watch

  - nonResourceURLs:
      - "/metrics"
    verbs:
      - get
---
{{- end }}
